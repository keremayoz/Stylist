{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"portrait_seg_train.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LiMpn-DZPt8I","colab_type":"text"},"source":["# DATASET INFORMATION"]},{"cell_type":"markdown","metadata":{"id":"tgqMZXXaPt8J","colab_type":"text"},"source":["*http://xiaoyongshen.me/webpage_portrait/index.html dataset su \n","* 1597 tane image ve pixel level mask var(800x600) size\n","*  1397 tanesi trainingte, 100 tanesi valde, 100 tanesi test kullanildi."]},{"cell_type":"code","metadata":{"id":"bH372JZpq4Jd","colab_type":"code","colab":{}},"source":["def meaniou(prediction,target):\n","    intersection = np.logical_and(target, prediction)\n","    union = np.logical_or(target, prediction)\n","    iou_score = np.sum(intersection) / np.sum(union)\n","    return iou_score\n","    \n","sum = 0\n","for i in range(len(predictions)):\n","    sum += meaniou(predictions[i], label[i])t\n","    \n","print(sum/len(predictions))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f5qM6EglPt8L","colab_type":"text"},"source":["# MODEL INFORMATION"]},{"cell_type":"markdown","metadata":{"id":"dXkTLEwbPt8M","colab_type":"text"},"source":["* Modelin yaptigi semantic segmentation deniliyor.\n","* Model architecture UNet ( Encoder ve Decoder parcalari var ve encoder ve decoder arasinda residual connectionlar var  featurelari almak icin encoder partindan.)\n","* Model encoder tarafinda MobilenetV2 denilen model var. Bu modeli kullanmamizin nedeni imagenetteki featurelari cikarma basarisi ve kucuk, efficient olmasi.\n","* Model decoder partini biz yazdik.MobileNetv2 ara layerlarindan feature mapleri aliyor( residual connection)\n","* Loss olarak iki lossun birlesimini kullandik. \n","**Dice Loss: Intersection Over Union Oranini artirmaya calisiyor.\n","<br>\n","-------------------code-----------------------------\n","modely.py icinde\n","----------------------------------------------------\n","<br>\n","**Binary Cross Entropy Loss:Output ve Target Masklerinin pixel similarity artirmaya calisiyor(pixel wise loss)(her pixel icin)\n","*Metric : Mean of Intersection Over Union Orani : bizim model %96.1 (MIou)\n","<br>\n","-------------------code-----------------------------\n","def meaniou(prediction,target):\n","    intersection = np.logical_and(target, prediction)\n","    union = np.logical_or(target, prediction)\n","    iou_score = np.sum(intersection) / np.sum(union)\n","    return iou_score\n","    \n","sum = 0\n","for i in range(len(predictions)):\n","    sum += meaniou(predictions[i], label[i])t\n","    \n","print(sum/len(predictions))\n","\n","--------------------------------------------------------\n","    \n","<br>\n","*Nvidia 950M : ~15-20 FPS\n","*Skype Blur ozelligi buna benzer bir AI kullaniyor."]},{"cell_type":"markdown","metadata":{"id":"JxsbdwdarJ6J","colab_type":"text"},"source":["#PREPROCESSING\n","\n","*Imagelerin verilen bound gore croplanmasi gerekiyordu. Bir resim 1200x900 olabiliyor ama tum masklar 800x600 boyutunda"]},{"cell_type":"code","metadata":{"id":"1nf4RcFbq_hJ","colab_type":"code","colab":{}},"source":["def load_crop_rects(filepath):\n","    # Load file\n","    \n","    with open(filepath, 'r') as f:\n","        lines = f.read().strip().split('\\n')\n","\n","    rect_pairs = list()\n","    for line in lines:\n","        items = line.split()\n","        if len(items) != 5:\n","           \n","            continue\n","      \n","        rect_pairs.append((items[0], items[1:5]))\n","\n","    return rect_pairs\n","\n","def crop_img(img_name, src_dir, dst_dir, crop_rect, img_size):\n","    src_path = os.path.join(src_dir, img_name)\n","    if not os.path.exists(src_path):\n","        \n","        return\n","\n","    dst_path = os.path.join(dst_dir, img_name)\n","    if os.path.exists(dst_path):\n","        \n","        return\n","\n","    img = cv2.imread(src_path)\n","    x0, y0 = int(crop_rect[2]), int(crop_rect[0])\n","    x1, y1 = int(crop_rect[3]), int(crop_rect[1])\n","    img = img[y0:y1, x0:x1, :]\n","    img = cv2.resize(img, img_size)\n","    cv2.imwrite(dst_path, img)\n","    print(dst_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZtPVxS3sOpZ","colab_type":"text"},"source":["# Random Cropping\n"," \n"," * Model sadece square size ( m x m ) image kabul ettigi icin 512x512 random crop aldik her image ve masktan 5 tane elde ettik toplamda 1597 * 5 dataset oldu. 512x512 secmemizin nedeni 32 bolunmesi ( maxpooling icin) ve precision icin buyuk imageler icin.Normalde pretrained model ( MobilenetV2 224x224 aliyor)"]},{"cell_type":"code","metadata":{"id":"tycwh32ysSqF","colab_type":"code","colab":{}},"source":["import random\n","count =  1 \n","names = random.sample(range(int(1e10)), len(imgs) * 20)\n","\n","os.system(\"mkdir augtrain_data\")\n","os.system(\"mkdir augtrain_label\")\n","for i in range(len(imgs)-200):\n","    for k in range(5):\n","        j = imgs_mask[i]\n","        rand1 = np.random.randint(j.shape[0]-513)\n","        rand2 = np.random.randint(j.shape[1]-513)\n","        a = cv2.cvtColor(imgs[i], cv2.COLOR_RGB2BGR)\n","        cv2.imwrite(\"./augtrain_data/{}.png\".format(names[count]),a[rand1:rand1+512,rand2:rand2+512,:])\n","        cv2.imwrite(\"./augtrain_label/{}.png\".format(names[count]),j[rand1:rand1+512,rand2:rand2+512])\n","        count +=1\n","        \n","os.system(\"mkdir augval_data\")\n","os.system(\"mkdir augval_label\")        \n","for i in range(len(imgs)-200, len(imgs) -100):\n","    for k in range(5):\n","        j = imgs_mask[i]\n","        rand1 = np.random.randint(j.shape[0]-513)\n","        rand2 = np.random.randint(j.shape[1]-513)\n","        a = cv2.cvtColor(imgs[i], cv2.COLOR_RGB2BGR)\n","        cv2.imwrite(\"./augval_data/{}.png\".format(names[count]),a[rand1:rand1+512,rand2:rand2+512,:])\n","        cv2.imwrite(\"./augval_label/{}.png\".format(names[count]),j[rand1:rand1+512,rand2:rand2+512])\n","        count +=1\n","        \n","os.system(\"mkdir augtest_data\")\n","os.system(\"mkdir augtest_label\")        \n","for i in range(len(imgs)-100, len(imgs)):\n","    for k in range(5):\n","        j = imgs_mask[i]\n","        rand1 = np.random.randint(j.shape[0]-513)\n","        rand2 = np.random.randint(j.shape[1]-513)\n","        a = cv2.cvtColor(imgs[i], cv2.COLOR_RGB2BGR)\n","        cv2.imwrite(\"./augtest_data/{}.png\".format(names[count]),a[rand1:rand1+512,rand2:rand2+512,:])\n","        cv2.imwrite(\"./augtest_label/{}.png\".format(names[count]),j[rand1:rand1+512,rand2:rand2+512])\n","        count +=1\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTxXwBJklf6o","colab_type":"text"},"source":["# FURTHER DATA AUGMENTATION \n"," \n"," *Image pixelleri rescale edildi img /= 255.0 --> pixel degerleri [0,1], amac weightlerin stabilitisi( masklerede uygulandi)\n","<br>\n","** Brightness Augmentation : Color Space HSV cevirildi, brightness parti oldugu icin bu color space kullanildi. Random sayilarla carpilip, datalarin brightness degerleri degistirildi. Tekrardan HSV den RGB color space donuldu.Amac farkli isik conditionlarina alistirmak\n","<br>\n","** Quality Augmentation : Opencv kullanilarak resim okunup memory bufferina random olarak dusuk quality olarak kaydedildi. Sonra bufferdan okundu. Amac farkli qualitydeki resimlere alistirmak. Ayni size quality dusurdum.\n","\n"]},{"cell_type":"code","metadata":{"id":"SNpvBV8TsbND","colab_type":"code","colab":{}},"source":["img = cv2.cvtColor(cv2.imread(\"test3.jpg\"),cv2.COLOR_BGR2RGB)\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","def brightness_adjustment(img):\n","    #print(img.shape)\n","    if(img.shape[2] == 3):\n","      # turn the image into the HSV space\n","      hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n","      # creates a random bright\n","    \n","      ratio = .5 + np.random.uniform(-0.499,+0.499)\n","      # convert to int32, so you don't get uint8 overflow\n","      # multiply the HSV Value channel by the ratio\n","      # clips the result between 0 and 255\n","      # convert again to uint8\n","      hsv[:,:,2] =  np.clip(hsv[:,:,2].astype(np.int32) * ratio, 0, 255).astype(np.uint8)\n","      # return the image int the BGR color space\n","      return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB).astype(np.uint8)\n","\n","\n","def img_compression(img):\n","    if img.shape[2] == 3:\n","    \n","      encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50 + np.random.uniform(-40,20)]\n","      result, encimg = cv2.imencode('.jpg', img, encode_param)\n","      img = cv2.imdecode(encimg, 1)\n","      \n","      return brightness_adjustment( img).astype(np.float64)\n","    return img\n","\n","plt.imshow(img_compression(img).astype(np.uint8))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"67qfanQXPt8M","colab_type":"text"},"source":["# TRAINING"]},{"cell_type":"code","metadata":{"id":"7WoLNiCHlZE9","colab_type":"code","colab":{}},"source":["from modely import *\n","from keras.models import load_model\n","import random\n","size = 512\n","model = unet(input_size=(size,size,3) )#load_model(\"human_weights-004-0.1119.hdf5\", custom_objects={\"awesomeq_loss\":awesomeq_loss})#unet(None,input_size = (size,size,3))\n","import os\n","import cv2\n","fname = \"human_weights-{epoch:03d}-{val_loss:.4f}.hdf5\"\n","from keras.callbacks import ModelCheckpoint\n","checkpoint = ModelCheckpoint(fname, monitor=\"val_loss\", save_best_only=True, verbose=1)\n","callbacks = [checkpoint]\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","batch = 16\n","\n","\n","def brightness_adjustment(img):\n","    #print(img.shape)\n","    if(img.shape[2] == 3):\n","      # turn the image into the HSV space\n","      hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n","      # creates a random bright\n","      ratio = .5 + np.random.uniform(-0.499,0.799)\n","      # convert to int32, so you don't get uint8 overflow\n","      # multiply the HSV Value channel by the ratio\n","      # clips the result between 0 and 255\n","      # convert again to uint8\n","      hsv[:,:,2] =  np.clip(hsv[:,:,2].astype(np.int32) * ratio, 0, 255).astype(np.uint8)\n","      # return the image int the BGR color space\n","      return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB).astype(np.uint8)\n","    \n","    return img\n","  \n","def img_compression(img):\n","    if img.shape[2] == 3:\n","    \n","      encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50 + np.random.uniform(-40,20)]\n","      result, encimg = cv2.imencode('.jpg', img, encode_param)\n","      img = cv2.imdecode(encimg, 1)\n","      \n","      return brightness_adjustment( img).astype(np.float64)\n","    return img\n","\n","data_gen_args = dict(preprocessing_function= img_compression,rescale = 1/255.0)\n","image_datagen = ImageDataGenerator(**data_gen_args)\n","mask_datagen = ImageDataGenerator(**data_gen_args)\n","seed= 1\n","\n","image_generator = image_datagen.flow_from_directory(\n","    'train',\n","    class_mode=None,\n","    target_size = (size,size),\n","    classes = ['data'],\n","    color_mode ='rgb',\n","    batch_size = batch,\n","    shuffle=True,\n","    seed=seed)\n","\n","mask_generator = mask_datagen.flow_from_directory(\n","    'train',\n","    class_mode=None,\n","    classes = ['label_1'],\n","    target_size = (size,size),\n","    color_mode = 'grayscale',\n","    batch_size = batch,\n","    \n","    seed=seed)\n","\n","train_generator = zip(image_generator, mask_generator)\n","\n","vimage_generator = image_datagen.flow_from_directory(\n","    'val',\n","    class_mode=None,\n","    classes = ['data'],\n","    target_size = (size,size),\n","    color_mode ='rgb',\n","    batch_size = batch,\n","    shuffle = True,\n","    seed=seed)\n","\n","vmask_generator = mask_datagen.flow_from_directory(\n","    'val',\n","    class_mode=None,\n","    classes = ['label_1'],\n","    target_size = (size,size),\n","    color_mode = 'grayscale',\n","    batch_size = batch, \n","    seed=seed)\n","val_generator = zip(vimage_generator, vmask_generator)\n","\n","from keras.optimizers import SGD,Adam, RMSprop\n","\n","optimizer =  RMSprop(5e-5)\n","model.compile(optimizer = Adam(lr = 5e-5), loss = awesomeq_loss, metrics = ['accuracy'])\n","\n","\n","H = model.fit_generator(\n","        train_generator,\n","\t      callbacks = callbacks,\n","        steps_per_epoch=6985//batch,\n","        epochs=20,\n","        verbose = 1,\n","        validation_data=val_generator,\n","        validation_steps=500//batch)\n","\n","import numpy as np \n","import matplotlib.pyplot as plt \n","\n","plt.style.use(\"ggplot\")\n","fig = plt.figure()\n","\n","\n","plt.plot(np.arange(0, 20), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, 20), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, 20), H.history[\"acc\"], label=\"train_acc\")\n","plt.plot(np.arange(0, 20), H.history[\"val_acc\"], label=\"val_acc\")\n","title = \"0.01\"\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch#\")\n","\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend()\n","fig.savefig('image.png', dpi=fig.dpi)\n","plt.show()"],"execution_count":0,"outputs":[]}]}